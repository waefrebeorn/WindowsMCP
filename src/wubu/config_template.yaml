# WuBu Configuration File (Template)
# This file contains example configurations for WuBu subsystems.
# Copy this to 'wubu_config.yaml' at the project root (or src/) and customize.

# General WuBu Settings
wubu_name: "WuBu"
profile: "default" # Allows for different WuBu personalities or configurations

# TTS (Text-to-Speech) Configuration
tts:
  default_voice: "zonos_engine_local_cloning" # Preferred default voice ID. Options: "zonos_engine_local_cloning", "WuBu-GLaDOS-Style", "WuBu-Kokoro"
  estimated_max_speech_duration: 7 # Seconds, used by CLI for waiting after direct input.

  # Configuration for ZonosLocalVoice (Local Zonos, high-quality, voice cloning)
  # Requires successful setup of Zonos dependencies (Python packages, eSpeak-NG, CUDA if using GPU).
  zonos_local_engine:
    enabled: true # Set to true to enable Local Zonos TTS
    language: 'en' # Default language for Zonos (e.g., "en", "ja", "zh", "fr", "de")
                    # These map to Zonos specific codes like "en-us", "ja-jp", etc.
    # model_id: Hugging Face model ID or local path for Zonos.from_pretrained
    model_id: "Zyphra/Zonos-v0.1-transformer"
    # device: "cuda" or "cpu". ZonosLocalVoice will attempt to use ZONOS_DEFAULT_DEVICE from zonos_local_lib if not set.
    # It's recommended to set this explicitly. Ensure CUDA environment is correctly set up if using "cuda".
    device: "cpu" # Change to "cuda" for GPU acceleration if available and configured.
    # Optional: Path to a default .wav audio file for speaker voice cloning.
    # ZonosLocalVoice uses this for its default_voice if provided.
    # Use forward slashes (/) or properly escaped backslashes (\\) for YAML paths.
    default_reference_audio_path: "" # e.g., "assets/voices/my_local_speaker.wav"
    # Optional: Configuration for unconditional keys during conditioning.
    # Default in ZonosLocalVoice is ["emotion"] to match Gradio's typical behavior.
    # unconditional_keys: ["vqscore_8", "dnsmos_ovrl"] # Example from Zonos docs for some models for less emotional output.
    # Optional: Further Zonos specific parameters can be added here if ZonosLocalVoice is extended
    # to read them from its config (e.g., cfg_scale, sampling_params).
    # cfg_scale: 2.0
    # sampling_params:
    #   min_p: 0.1
    # progress_bar: false # Usually false for backend synthesis

  # Configuration for WubuGLaDOSStyleVoice (Coqui XTTSv2 based characteristic voice)
  wubu_glados_style_voice:
    enabled: true # Set to false if not using or if Zonos is preferred.
    language: 'en'
    # model_subdir: name of the directory under src/wubu/tts/ that holds the model files
    # e.g., "glados_tts_models" if you have src/wubu/tts/glados_tts_models/
    model_subdir: 'glados_tts_models'
    # speaker_wav_filename: reference audio file within model_subdir for XTTSv2 voice cloning
    speaker_wav_filename: 'glados_reference.wav' # You'll need to provide this file
    use_gpu: true # true to use GPU for Coqui TTS if available, false for CPU

  # Configuration for WubuKokoroVoice (Standard/Neutral voice)
  wubu_kokoro_voice:
    enabled: true # Set to false if not using or if Zonos is preferred.
    engine_type: 'coqui' # 'coqui' or 'piper' (if Piper backend is implemented in WubuKokoroVoice)
    language: 'en'
    # --- Coqui Settings for Kokoro (if engine_type is 'coqui') ---
    coqui_model_name: "tts_models/en/ljspeech/tacotron2-DDC" # Standard, downloadable Coqui model
    use_gpu: true

    # --- Piper Settings for Kokoro (if engine_type is 'piper' and implemented) ---
    # piper_model_filename: "en_US-standard-medium.onnx" # Example .onnx filename
    # piper_model_subdir: "kokoro_tts_models/piper_en_us_standard_medium" # Subdir in src/wubu/tts/

  # Example for ElevenLabs (Cloud TTS - Not implemented in current files)
  # elevenlabs_voice:
  #   enabled: false
  #   # Recommended: Set ELEVENLABS_API_KEY environment variable.
  #   # api_key: "YOUR_ELEVENLABS_API_KEY" # Or set here (less secure for shared configs)
  #   voice_id: "desired_elevenlabs_voice_id"


# ASR (Automatic Speech Recognition) Configuration
asr:
  enabled: false # Enable to use voice input features
  engine: "vosk" # Placeholder: "vosk", "whisper", "google_cloud_stt", etc.

  # Vosk specific settings (example)
  # Assumes models are in project_root/asr_models/vosk-model-small-en-us-0.15
  vosk_model_path: "asr_models/vosk-model-small-en-us-0.15"
  vosk_sample_rate: 16000

  # Whisper specific settings (example)
  # whisper_model_size: "base.en"
  # whisper_device: "cuda"

  # Common ASR settings
  audio_input_device_index: null # null for default, or integer for specific device
  silence_threshold: 1.0


# LLM (Large Language Model) Configuration
llm:
  provider: "ollama" # "ollama", "openai", "huggingface_hub", "anthropic", etc.

  ollama_settings:
    model: "phi:latest"
    host: "http://localhost:11434"
    # request_timeout: 60

  # openai_settings:
  #   # Recommended: Set OPENAI_API_KEY environment variable.
  #   api_key: "YOUR_OPENAI_API_KEY" # Or set here (less secure for shared configs)
  #   model: "gpt-3.5-turbo"
  #   temperature: 0.7
  #   max_tokens: 150

# Vision Subsystem Configuration (Placeholder)
vision:
  enabled: false
  # screen_capture_monitor: 0
  # object_detection_model: "yolov5s"

# Desktop Tools Configuration (Integration with existing desktop_tools package)
desktop_tools:
  enabled: true # Whether WuBu can use desktop interaction tools from desktop_tools/
  # Specific tool configurations can go here if needed (e.g. default browser for web_interaction)

# Logging Configuration
logging:
  level: "INFO" # "DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL"
  log_file: "wubu.log" # Path to log file (relative to project root), null for console only
  max_log_size_mb: 10
  backup_count: 3
